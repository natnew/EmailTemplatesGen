# Evaluating data

# Import necessary libraries
import pandas as pd
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from nltk.translate.bleu_score import sentence_bleu

# Load preprocessed data
cleaned_data = pd.read_csv('./data/processed/cleaned_data.csv')

# Load trained GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('models/trained_gpt2_model')
tokenizer = GPT2Tokenizer.from_pretrained('models/trained_gpt2_tokenizer')

# Set pad token to eos token and padding side to left
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'

# Prepare evaluation data
texts = [" ".join(tokens) for tokens in cleaned_data['clean_text']]
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Generate predictions
model.eval()
with torch.no_grad():
    outputs = model.generate(inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device), max_new_tokens=100)

# Decode predictions
predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

# Example reference text for BLEU score evaluation
reference_texts = ["I hope this email finds you well."] * len(predictions)  # Ensure the list is long enough

# Calculate BLEU scores
for i, prediction in enumerate(predictions):
    reference = [reference_texts[i].split()]  # Tokenize reference text
    candidate = prediction.split()  # Tokenize generated text
    bleu_score = sentence_bleu(reference, candidate)
    print(f"Prediction {i+1}: {prediction}")
    print(f"BLEU score: {bleu_score}")
