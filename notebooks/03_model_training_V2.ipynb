{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Project1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Check current working directory\n",
    "files = r\"C:/Project1/ForkNatNew/data/processed/\"\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load preprocessed data\n",
    "cleaned_data = pd.read_csv(r\"C:/Project1/ForkNatNew/data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token # This line sets the padding token to the end-of-sequence token, which resolves the issue of the tokenizer not having a padding token.\n",
    "\n",
    "# Example input (you would use cleaned data for actual training)\n",
    "sample_text = \" \".join(cleaned_data['clean_text'].iloc)\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512) # This line tokenizes the input text with padding and truncation, ensuring that the input length does not exceed the specified max_length (512 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Train the model (expand this section to include full training loop)\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100) # This line generates the output from the model, using the attention_mask to handle padding correctly. The max_new_tokens parameter specifies the maximum number of new tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in 'models/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained('models/trained_gpt2_model')\n",
    "tokenizer.save_pretrained('models/trained_gpt2_tokenizer')\n",
    "\n",
    "print(\"Model and tokenizer saved in 'models/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train data in a loop\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load preprocessed data\n",
    "cleaned_data = pd.read_csv(r\"C:/Project1/ForkNatNew/data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a custom dataset class\n",
    "class EmailDataset(Dataset): # This class inherits from torch.utils.data.Dataset - used to prepare data for training. It tokenizes the text data and returns the input tensors.\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        inputs['labels'] = inputs.input_ids.clone()\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset and dataloader\n",
    "texts = [\" \".join(tokens) for tokens in cleaned_data['clean_text']] # texts: Combines the cleaned tokens into text strings.\n",
    "dataset = EmailDataset(texts, tokenizer, max_length=512) # dataset: Creates an instance of the EmailDataset class.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # dataloader: Creates a DataLoader to handle batching and shuffling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\humphrys\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) # optimizer: Uses the AdamW optimizer with a learning rate of 5e-5.\n",
    "total_steps = len(dataloader) * 3  # Assuming 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # scheduler: Uses a linear learning rate scheduler with warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.2062025666236877\n",
      "Epoch 2/3, Loss: 1.521947205066681\n",
      "Epoch 3/3, Loss: 1.4388502836227417\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# The training loop runs for a specified number of epochs (3 in this example).\n",
    "# For each batch, it performs the following steps: Zeroes the gradients.\n",
    "# Moves the input tensors to the GPU if available.\n",
    "# Computes the model outputs and loss.\n",
    "# Backpropagates the loss.\n",
    "# Updates the model parameters.\n",
    "# Adjusts the learning rate using the scheduler.\n",
    "# Prints the average loss for each epoch.\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        labels = batch['labels'].squeeze().to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in 'models/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and tokenizer to the 'models/' directory.\n",
    "model.save_pretrained('models/trained_gpt2_model')\n",
    "tokenizer.save_pretrained('models/trained_gpt2_tokenizer')\n",
    "\n",
    "print(\"Model and tokenizer saved in 'models/' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
