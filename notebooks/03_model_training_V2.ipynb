# Import necessary libraries
import pandas as pd
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load preprocessed data
cleaned_data = pd.read_csv('./data/processed/cleaned_data.csv')

# Load GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Set pad token to eos token
tokenizer.pad_token = tokenizer.eos_token # This line sets the padding token to the end-of-sequence token, which resolves the issue of the tokenizer not having a padding token.

# Example input (you would use cleaned data for actual training)
sample_text = " ".join(cleaned_data['clean_text'].iloc)
inputs = tokenizer(sample_text, return_tensors="pt", padding=True, truncation=True, max_length=512) # This line tokenizes the input text with padding and truncation, ensuring that the input length does not exceed the specified max_length (512 in this case).

# Train the model (expand this section to include full training loop)
outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100) # This line generates the output from the model, using the attention_mask to handle padding correctly. The max_new_tokens parameter specifies the maximum number of new tokens to generate.

# Save the trained model
model.save_pretrained('models/trained_gpt2_model')
tokenizer.save_pretrained('models/trained_gpt2_tokenizer')

print("Model and tokenizer saved in 'models/' directory.")



#Train data in a loop

# Import necessary libraries
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup

# Load preprocessed data
cleaned_data = pd.read_csv('/content/drive/My Drive/data/processed/cleaned_data.csv')

# Load GPT-2 model and tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Set pad token to eos token
tokenizer.pad_token = tokenizer.eos_token

# Define a custom dataset class
class EmailDataset(Dataset): # This class inherits from torch.utils.data.Dataset - used to prepare data for training. It tokenizes the text data and returns the input tensors.
    def __init__(self, texts, tokenizer, max_length):
        self.texts = texts 
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        inputs = self.tokenizer(text, return_tensors="pt", padding='max_length', truncation=True, max_length=self.max_length)
        inputs['labels'] = inputs.input_ids.clone()
        return inputs

# Prepare the dataset and dataloader
texts = [" ".join(tokens) for tokens in cleaned_data['clean_text']] # texts: Combines the cleaned tokens into text strings.
dataset = EmailDataset(texts, tokenizer, max_length=512) # dataset: Creates an instance of the EmailDataset class.
dataloader = DataLoader(dataset, batch_size=2, shuffle=True) # dataloader: Creates a DataLoader to handle batching and shuffling of the data.

# Set up the optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=5e-5) # optimizer: Uses the AdamW optimizer with a learning rate of 5e-5.
total_steps = len(dataloader) * 3  # Assuming 3 epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # scheduler: Uses a linear learning rate scheduler with warmup.

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Training loop
# The training loop runs for a specified number of epochs (3 in this example).
# For each batch, it performs the following steps: Zeroes the gradients.
# Moves the input tensors to the GPU if available.
# Computes the model outputs and loss.
# Backpropagates the loss.
# Updates the model parameters.
# Adjusts the learning rate using the scheduler.
# Prints the average loss for each epoch.

epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].squeeze().to(device)
        attention_mask = batch['attention_mask'].squeeze().to(device)
        labels = batch['labels'].squeeze().to(device)
        
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        
        loss.backward()
        optimizer.step()
        scheduler.step()
    
    avg_loss = total_loss / len(dataloader)
    print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}")

# Save the trained model and tokenizer to the 'models/' directory.
model.save_pretrained('models/trained_gpt2_model')
tokenizer.save_pretrained('models/trained_gpt2_tokenizer')

print("Model and tokenizer saved in 'models/' directory.")
